{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.dataset import FerDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualUnit(nn.Module):\n",
    "    \n",
    "    def __init__(self, depth_in, depth_out):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.stride = 1\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.resBlock = nn.Sequential(\n",
    "            nn.Conv2d(depth_in, depth_out, kernel_size=(3, 3), stride = self.stride, padding = 1),\n",
    "            nn.BatchNorm2d(depth_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(depth_out, depth_out, kernel_size=(3, 3), stride = self.stride, padding = 1),\n",
    "            nn.BatchNorm2d(depth_out)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Residual unit\n",
    "        identity = x\n",
    "        \n",
    "        result = self.resBlock(x)\n",
    "        #Residual unit\n",
    "        #print(\"x.shape\", x.shape)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        \n",
    "        result += identity\n",
    "        result = self.relu(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DownsampleResidualUnit(nn.Module):\n",
    "    \n",
    "    def __init__(self, depth_in, depth_out):\n",
    "        super(DownsampleResidualUnit, self).__init__()\n",
    "        self.stride = 2\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.resBlock = nn.Sequential(\n",
    "            nn.Conv2d(depth_in, depth_out, kernel_size=(3, 3), stride = self.stride, padding = 1),\n",
    "            nn.BatchNorm2d(depth_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(depth_out, depth_out, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(depth_out)\n",
    "        )\n",
    "        \n",
    "        self.matchDim = nn.Sequential(\n",
    "            nn.Conv2d(depth_in, depth_out, kernel_size=(1,1), stride=self.stride, padding = 0),\n",
    "            # this is required to match the dimensions of the identity x with F(x), because\n",
    "            # in this block the first of the two convolutionl layers performs downsamlpling and therefore\n",
    "            # changes the dimensions of the activation volume.\n",
    "            nn.BatchNorm2d(depth_out)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Downsample unit\n",
    "        identity = x\n",
    "        \n",
    "        result = self.resBlock(x)\n",
    "        #Downsample unit\n",
    "        #print(\"x.shape\", x.shape)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        \n",
    "        \n",
    "        identity = self.matchDim(identity)\n",
    "        \n",
    "        \n",
    "        result += identity\n",
    "        result = self.relu(result)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "# https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            # 224 x 224 x 1\n",
    "            nn.Conv2d(1, 64, kernel_size=(7, 7), stride = 2, padding = 3),\n",
    "            # 7 x 7, out_depth=64, stride=2 are given by the paper. The output has to be 112 x 112 x 64.\n",
    "            # Therefore the padding ist calculated via: P = ((width_out-1) * stride + filter_size - width_in)/2\n",
    "            # instead of width_in and width_out one can also use height_in and height_out\n",
    "            # ((112-1)*2+7-224)/2 = P = 2.5, i.e. we choose padding = 3\n",
    "            \n",
    "            # 112 x 112 x 64\n",
    "            \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=2, padding = 1),\n",
    "            # MaxPool with kernel = (3,3) and stride=2 is given by the paper\n",
    "            # P = ((56-1) * 2 + 3 - 112)/2 = 0.5, i.e. P=1\n",
    "            \n",
    "            # 56 x 56 x 64\n",
    "            ResidualUnit(64, 64),\n",
    "            ResidualUnit(64, 64), \n",
    "            ResidualUnit(64, 64), \n",
    "            \n",
    "            DownsampleResidualUnit(64, 128),\n",
    "            # 28 x 28 x 128\n",
    "            ResidualUnit(128, 128),\n",
    "            ResidualUnit(128, 128),\n",
    "            ResidualUnit(128, 128),\n",
    "            \n",
    "            DownsampleResidualUnit(128, 256),\n",
    "            # 14 x 14 x 256\n",
    "            ResidualUnit(256, 256),\n",
    "            ResidualUnit(256, 256),\n",
    "            ResidualUnit(256, 256),\n",
    "            ResidualUnit(256, 256),\n",
    "            ResidualUnit(256, 256),\n",
    "            \n",
    "            DownsampleResidualUnit(256, 512),\n",
    "            # 7 x 7 x 512\n",
    "            ResidualUnit(512, 512),\n",
    "            ResidualUnit(512, 512),\n",
    "            \n",
    "            #nn.AdaptiveAvgPool2d(1)  \n",
    "            nn.AvgPool2d(kernel_size = (7,7), stride=1, padding=0)         \n",
    "            # width_out = (width_in - F + 2P)/S+1\n",
    "            # = (7 - 7 + 0)/2 + 1 = 1\n",
    "            # 1 x 1 x 512\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 10)        \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        #print(\"x.shape\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i have to check dimension and padding of: the first convolutional layer and the max pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = FerDataset(base_path='../../data',\n",
    "                     data='ferplus',\n",
    "                     mode='train',\n",
    "                     label='ferplus_votes')\n",
    "dataloader = DataLoader(dataset, batch_size=6, shuffle=True, num_workers=0)\n",
    "net = ResNet()\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "criterion = nn.KLDivLoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_img(x_batch, y_batch, size):\n",
    "    x_batch_resized = torch.zeros((x_batch.shape[0], x_batch.shape[1], size, size))\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        image = torchvision.transforms.ToPILImage()(x_batch[i])\n",
    "        image = torchvision.transforms.functional.resize(image, (size, size))\n",
    "        x_batch_resized[i] = torchvision.transforms.ToTensor()(image)\n",
    "        \n",
    "    return x_batch_resized, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 48, 48])\n",
      "torch.Size([6, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(dataloader))\n",
    "print(x_batch.shape)\n",
    "x_batch, y_batch = resize_img(x_batch, y_batch, 224)\n",
    "print(x_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\r"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "#for i in range(1000):\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    logits = net(x_batch)\n",
    "    log_probs = log_softmax(logits)\n",
    "    loss = criterion(log_probs, y_batch)\n",
    "    losses.append(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
