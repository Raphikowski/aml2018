{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.dataset import FerDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG13(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGG13, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            # 48 x 48 x 1\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            # kernel size F=3, stride S = 1, to retain input size padding must be P = (F - 1)/2\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # 24 x 24 x 64\n",
    "            \n",
    "            # max pool with F=2 and S=2 chooses the max out of a 2x2 square and only keeps that max value.\n",
    "            # Therefore 75% of the information are left out\n",
    "            # the max pool layer works on every depth dimension independently, therefore the input depth remains\n",
    "            # unchanged\n",
    "            \n",
    "            #nn.Dropout2d(p=0.25),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # 12 x 12 x 128\n",
    "\n",
    "            #nn.Dropout2d(p=0.25),\n",
    "\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # 6 x 6 x 256\n",
    "\n",
    "            #nn.Dropout2d(p=0.25),\n",
    "\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # 3 x 3 x 512\n",
    "            \n",
    "            #nn.Dropout2d(p=0.25),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # 3 x 3 x 512\n",
    "            \n",
    "            #nn.Dropout2d(p=0.25)\n",
    "            \n",
    "            #nn.AvgPool2d(kernel_size = (3,3), stride=3, padding=3)\n",
    "            # This layer is not in the pytorch implementation of VGG13 and neither in the paper\n",
    "            # 3 x 3 x 512\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            #nn.Linear(512*7*7, 4096), without the AvgPool2d layer\n",
    "            nn.Linear(512*3*3, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,10),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        print(\"x.shape\", x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = FerDataset(base_path='../../data',\n",
    "                     data='ferplus',\n",
    "                     mode='train',\n",
    "                     label='ferplus_votes')\n",
    "dataloader = DataLoader(dataset, batch_size=6, shuffle=True, num_workers=0)\n",
    "net = VGG13()\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "criterion = nn.KLDivLoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_img(x_batch, y_batch, size):\n",
    "    x_batch_resized = torch.zeros((x_batch.shape[0], x_batch.shape[1], size, size))\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        image = torchvision.transforms.ToPILImage()(x_batch[i])\n",
    "        # PIL image of size 48 x 48\n",
    "        #image = torchvision.transforms.functional.resize(image, (size, size))\n",
    "        image = image.resize((size, size), resample=Image.NEAREST) # use fct PIL.Image.resize\n",
    "        # PIL image of shape size x size\n",
    "        x_batch_resized[i] = torchvision.transforms.ToTensor()(image)\n",
    "        # Torch tensor of shape 1 x size x size\n",
    "        \n",
    "    return x_batch_resized, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 48, 48])\n",
      "torch.Size([6, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(dataloader))\n",
    "print(x_batch.shape)\n",
    "x_batch, y_batch = resize_img(x_batch, y_batch, 224)\n",
    "print(x_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    logits = net(x_batch)\n",
    "    log_probs = log_softmax(logits)\n",
    "    loss = criterion(log_probs, y_batch)\n",
    "    losses.append(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1, 224, 224])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
