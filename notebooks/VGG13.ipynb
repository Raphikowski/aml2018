{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.dataset import FerDataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG13(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGG13, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            # kernel size F=3, stride S = 1, to retain input size padding must be P = (F - 1)/2\n",
    "            nn.ReLU(),\n",
    "# sollte man ReLU immer nach jeder Conv layer einfuegen?\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            # max pool with F=2 and S=2 chooses the max out of a 2x2 square and only keeps that max value.\n",
    "            # Therefore 75% of the information are left out\n",
    "            # the max pool layer works on every depth dimension independently, therefore the input depth remains\n",
    "            # unchanged\n",
    "            \n",
    "            #nn.Dropout2d(p=0.25),\n",
    "# im Paper steht \"a drop out rate of 25%\". Heisst, dass p=0.25 oder p=0.75\n",
    "# und warum genau gibt es in dieser Architektur eine extra dropout layer? Normalerweise dachte ich, sollte jede conv\n",
    "# layer eine dropout layer sein um overfitten zu reduzieren oder?\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            #nn.Dropout2d(p=0.25),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            #nn.Dropout2d(p=0.25),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            #nn.Dropout2d(p=0.25),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            #nn.Dropout2d(p=0.25)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "# angenommen nn.Linear(512, 4096) ist richtig (es kommt zumindest kein Fehler) warum sollte der Input der FC layer\n",
    "# so gross sein wie die Anzahl an filtern der convolutional layer davor? Ich denke der input einer FC layer sollte\n",
    "# heigh x width x depth der convolutional layer davor sein oder?\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "# ist die Linear layer eine fully connected layer? Und was bringen zwei davon hinterinenander?\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,10),\n",
    "            #nn.Softmax(10)\n",
    "# ich verwende unten die loss Funktionen von dir, also die softmax funktion. Muss ich dann trotzdem\n",
    "# hier noch eine Softmax layer einfuegen? Weil du hast das nicht gemacht beim LeNet\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "# Was macht dieses funktion view()?\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = FerDataset(base_path='../../data',\n",
    "                     data='ferplus',\n",
    "                     mode='train',\n",
    "                     label='ferplus_votes')\n",
    "dataloader = DataLoader(dataset, batch_size=24, shuffle=True, num_workers=0)\n",
    "net = VGG13()\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "criterion = nn.KLDivLoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def resize_img(x_batch, y_batch, size):\n",
    "    x_batch_resized = torch.zeros((x_batch.shape[0], x_batch.shape[1], size, size))\n",
    "    for i in range(x_batch.shape[0]):\n",
    "        image = torchvision.transforms.ToPILImage()(x_batch[i])\n",
    "        image = torchvision.transforms.functional.resize(image, (size, size))\n",
    "        x_batch_resized[i] = torchvision.transforms.ToTensor()(image)\n",
    "        \n",
    "    return x_batch_resized, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 48, 48])\n",
      "torch.Size([24, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = next(iter(dataloader))\n",
    "print(x_batch.shape)\n",
    "resized_img, y = resize_img(x_batch, y_batch, 224)\n",
    "print(resized_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a6f5245c9d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_homework/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml_homework/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    logits = net(x_batch)\n",
    "    log_probs = log_softmax(logits)\n",
    "    loss = criterion(log_probs, y_batch)\n",
    "    losses.append(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vgg13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function vgg13 at 0x7fbf71547598>\n"
     ]
    }
   ],
   "source": [
    "print(vgg13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
